{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a33744a8-08c7-4158-a22f-e8f2008a25b0",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "412b006d19b0209bd10ee4c6d15fdf59f8d2af38",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.6.5 |Anaconda custom (64-bit)| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "pandas version: 0.23.0\n",
      "matplotlib version: 2.2.2\n",
      "NumPy version: 1.14.3\n",
      "SciPy version: 1.1.0\n",
      "IPython version: 6.4.0\n",
      "scikit-learn version: 0.19.1\n",
      "-------------------------\n",
      "creditcard.csv\n",
      "fundamentals.csv\n",
      "gender_submission.csv\n",
      "pima-indians-diabetes.data.csv\n",
      "prices.csv\n",
      "prices-split-adjusted.csv\n",
      "test.csv\n",
      "train.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "#load packages\n",
    "import sys #access to system parameters https://docs.python.org/3/library/sys.html\n",
    "print(\"Python version: {}\". format(sys.version))\n",
    "\n",
    "import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\n",
    "print(\"pandas version: {}\". format(pd.__version__))\n",
    "\n",
    "import matplotlib #collection of functions for scientific and publication-ready visualization\n",
    "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
    "\n",
    "import numpy as np #foundational package for scientific computing\n",
    "print(\"NumPy version: {}\". format(np.__version__))\n",
    "\n",
    "import scipy as sp #collection of functions for scientific computing and advance mathematics\n",
    "print(\"SciPy version: {}\". format(sp.__version__)) \n",
    "\n",
    "import IPython\n",
    "from IPython import display #pretty printing of dataframes in Jupyter notebook\n",
    "print(\"IPython version: {}\". format(IPython.__version__)) \n",
    "\n",
    "import sklearn #collection of machine learning algorithms\n",
    "print(\"scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "#misc libraries\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('-'*25)\n",
    "\n",
    "\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a514e15b-8caa-4b2e-bb46-13521761ef27",
    "_uuid": "2e4810f3c85ffdee7d7382167e455baf1c320bc8"
   },
   "source": [
    "#  Load Data Modelling Libraries\n",
    "\n",
    "We will use the popular *scikit-learn* library to develop our machine learning algorithms. In *sklearn,* algorithms are called Estimators and implemented in their own classes. For data visualization, we will use the *matplotlib* and *seaborn* library. Below are common classes to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b230c790-a3f5-46ed-b351-57e96cc8fa61",
    "_uuid": "4a16d09256317518769f21bf9cc38726df8b0078",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Common Model Algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "#Configure Visualization Defaults\n",
    "#%matplotlib inline = show plots in Jupyter Notebook browser\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "c0928d7d-043b-4aa5-8113-e96fb42dbdc7",
    "_uuid": "d0afb79a714ea826208235d05afac4e3f60554db",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import data from file: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n",
    "# data = pd.read_csv('clean.csv')\n",
    "\n",
    "df_train = pd.read_csv(\"train_new.csv\")\n",
    "df_test = pd.read_csv(\"test_new.csv\")\n",
    "\n",
    "\n",
    "\n",
    "NUMERIC_COLUMNS=['Alone','Family Size','Sex','Pclass','Fare','FareBand','Age','TitleCat','Embarked'] #72\n",
    "ORIGINAL_NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Sex','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] #83\n",
    "REVISED_NUMERIC_COLUMNS=[ 'Family_Survival','Sex', 'Fare','Pclass','Age','SibSp','Parch','IsAlone','Title','Embarked'] #84\n",
    "\n",
    "#,'Sx_Cl_Survival','Sx_Em_Survival',\"Sx_Si_Survival\",\"Sx_Pa_Survival\"\n",
    "\n",
    "# create test and training data\n",
    "data_to_train = df_train[REVISED_NUMERIC_COLUMNS].fillna(-1000)\n",
    "\n",
    "X=data_to_train.values\n",
    "Y=df_train['Survived'].values\n",
    "Y = Y.reshape((Y.size,1))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "43ae9ddf-85b6-4abd-89a7-cfb0ff70ebad",
    "_uuid": "9f4b880b2e485e6bc4b6a64113b8f816e2319e85"
   },
   "source": [
    "## Dividing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "00d10c37-5d8a-4eaf-8fe8-25a0b58e0bfd",
    "_uuid": "e2ef29db7e31dc83c10238ee33ad4ad0ad426183",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.         0.13913574 1.         0.475      1.\n",
      "  0.         0.         0.792      1.        ]\n",
      " [0.5        1.         0.01522459 0.38492872 0.2625     0.\n",
      "  0.         1.         0.15667311 0.60869565]] (668, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test =  model_selection.train_test_split(X, Y, test_size=0.25,random_state=21, stratify=Y)\n",
    "print(X_train[:2,:],Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Neural Network\n",
    "Three layer layers with default values 50, 15, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def layer_sizes(X, Y):\n",
    "    \n",
    "    n_x = X.shape[1]\n",
    "    n_h = 50 # size of hidden layer\n",
    "    n2 = 15\n",
    "    n3 = 15\n",
    "    n_y = 1 # size of output layer\n",
    "    return (n_x, n_h, n2, n3, n_y)\n",
    "\n",
    "\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n2, n3, n_y):\n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "    W1 = np.random.randn(n_x,n_h)*0.005 \n",
    "    b1 = np.zeros((1,n_h))\n",
    "    W2 = np.random.randn(n_h,n2)*0.005 \n",
    "    b2 = np.zeros((1,n2))\n",
    "    W3 = np.random.randn(n2,n3)*0.005 \n",
    "    b3 = np.zeros((1,n3))\n",
    "    W4 = np.random.randn(n3,n_y)*0.0075 \n",
    "    b4 = np.zeros((1,n_y))    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                  \"W4\": W4,\n",
    "                  \"b4\": b4}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "# Data Standardization\n",
    "from sklearn import preprocessing\n",
    "def forward_propagation(X, parameters):\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    W4 = parameters[\"W4\"]\n",
    "    b4 = parameters[\"b4\"]\n",
    "    \n",
    "#     X=  preprocessing.scale(X,axis=1)   \n",
    "\n",
    "    Z1 = np.dot(X,W1)+b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(A1,W2)+b2\n",
    "    A2 = np.tanh(Z2)\n",
    "    Z3 = np.dot(A2,W3)+b3\n",
    "    A3 = np.tanh(Z3)\n",
    "    Z4 = np.dot(A3,W4)+b4\n",
    "    A4 = sigmoid(Z4)\n",
    "#     print(\"DB W1 = %s, b1 = %s, W2 = %s, b2 = %s, , W3 = %s, b3 = %s\"%(W1.shape,b1.shape,W2.shape,b2.shape,W3.shape,b3.shape))\n",
    "#     assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2,\n",
    "             \"Z3\": Z3,\n",
    "             \"A3\": A3,\n",
    "             \"Z4\": Z4,\n",
    "             \"A4\": A4}\n",
    "    \n",
    "    return A4, cache\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "def compute_cost(A4, Y, parameters):\n",
    "    m = Y.size # number of example\n",
    "    cost = np.sum((A4-Y)**2)/m\n",
    "#     cost = -np.sum(Y*np.log(A2))/m\n",
    "#     print(A2)\n",
    "#     print(m)\n",
    "#     logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2),1-Y)\n",
    "#     cost = - np.sum(logprobs) * (1./m)\n",
    "#     cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "#     assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost \n",
    "\n",
    "def StestpestDescent(parameters, cache, X, Y): \n",
    "    m = X.shape[0]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    W4 = parameters[\"W4\"]\n",
    "#     Z1 = cache[\"Z1\"]\n",
    "    Z2 = cache[\"Z2\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    A3 = cache[\"A3\"]\n",
    "    A4 = cache[\"A4\"]\n",
    "\n",
    "    d4 = (A4-Y) #* Z2*(1-Z2)\n",
    "#     print(\"debug d3 = \",d3.shape)\n",
    "    d3 = np.dot(d4,W4.T)*(1- np.power(A3,2))\n",
    "    d2 = np.dot(d3,W3.T)*(1- np.power(A2,2))\n",
    "    d1 = np.dot(d2,W2.T)*(1- np.power(A1,2))\n",
    "   \n",
    "#     print(\"DB A1 = %s W2 =%s d1 = %s\" %(A1.shape,W2.shape,d1.shape))\n",
    "    dW4 = np.dot(A3.T,d4)/m\n",
    "    db4 = np.sum(d4,axis=0,keepdims=True)/m\n",
    "    dW3 = np.dot(A2.T,d3)/m\n",
    "#     print(\"BD1 dW3 =%s, W3 = %s\"%(dW3.shape,W3.shape))\n",
    "    db3 = np.sum(d3,axis=0,keepdims=True)/m\n",
    "    dW2 = np.dot(A1.T,d2)/m\n",
    "#     print(\"BD2 dW2 =%s, W2 = %s\"%(dW2.shape,W2.shape))\n",
    "    db2 = np.sum(d2,axis=0,keepdims=True)/m\n",
    "#     print(\"DB3 dW1\",db2.T)\n",
    "#     print(\"DB3 d1=\",d1.T)\n",
    "    dW1 = np.dot(X.T,d1)/m\n",
    "#     print(\"debug dW1 =%s, W1 = %s\"%(dW1.shape,W1.shape))\n",
    "    db1 = np.sum(d1,axis=0,keepdims=True)/m\n",
    "#     print(\"debug db1 =%s, b1 = %s\"%(db1.shape,parameters[\"b1\"].shape))\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2,\n",
    "             \"dW3\": dW3,\n",
    "             \"db3\": db3,\n",
    "             \"dW4\": dW4,\n",
    "             \"db4\": db4}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = .75):\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    W4 = parameters[\"W4\"]\n",
    "    b4 = parameters[\"b4\"]\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    dW3 = grads[\"dW3\"]\n",
    "    db3 = grads[\"db3\"]\n",
    "    dW4 = grads[\"dW4\"]\n",
    "    db4 = grads[\"db4\"]\n",
    "    \n",
    "#     rate = .01#np.linspace(0,learning_rate = 5,1000)\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    W3 = W3 - learning_rate*dW3\n",
    "    b3 = b3 - learning_rate*db3\n",
    "    W4 = W4 - learning_rate*dW4\n",
    "    b4 = b4 - learning_rate*db4\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                  \"W4\": W4,\n",
    "                  \"b4\": b4}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, n2, n3, num_iterations = 10000, print_cost=False):\n",
    "\n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[-1]\n",
    "    \n",
    "\n",
    "    parameters = initialize_parameters(n_x, n_h, n2, n3, n_y)\n",
    "#     W1 = parameters[\"W1\"]\n",
    "#     b1 = parameters[\"b1\"]\n",
    "#     W2 = parameters[\"W2\"]\n",
    "#     b2 = parameters[\"b2\"]\n",
    "#     W3 = parameters[\"W3\"]\n",
    "#     b3 = parameters[\"b3\"]\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        A3, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A3, Y, parameters)\n",
    "        if cost <.11: break\n",
    "        grads = StestpestDescent(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads,learning_rate = .2 - 10**(-7)*i)\n",
    "        if print_cost and i % 5000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.250000\n",
      "Cost after iteration 5000: 0.236366\n",
      "Cost after iteration 10000: 0.236365\n",
      "Cost after iteration 15000: 0.236364\n",
      "Cost after iteration 19928: 0.109999\n"
     ]
    }
   ],
   "source": [
    "n_h, n2, n3 = 50, 20, 20\n",
    "parameters = nn_model(X_train, Y_train, n_h, n2, n3, num_iterations=100000, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 50, 20, 20 hidden units: Train set  85.48%, Test set 85.20%\n"
     ]
    }
   ],
   "source": [
    "def predict(parameters, X):\n",
    "    A3, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A3 >= 0.5)\n",
    "    return np.array(predictions)\n",
    "train_predictions = predict(parameters, X_train)\n",
    "test_predictions = predict(parameters, X_test)\n",
    "train_accuracy = float((np.dot(Y_train[:,0],train_predictions) + np.dot(1-Y_train[:,0],1-train_predictions))/float(Y_train.size)*100)\n",
    "test_accuracy = float((np.dot(Y_test[:,0],test_predictions) + np.dot(1-Y_test[:,0],1-test_predictions))/float(Y_test.size)*100)\n",
    "print (\"Accuracy for {}, {}, {} hidden units: Train set  {:.2f}%, Test set {:.2f}%\".format(n_h, n2, n3, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some other prediction using other settings of Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden units: 50 30 20\n",
      "Cost after iteration 16686: 0.109999\n",
      "  Accuracy for 50, 30, 20 hidden units: Train set 85.48%, Test set 85.20%\n",
      "Hidden units: 60 30 30\n",
      "Cost after iteration 15037: 0.109998\n",
      "  Accuracy for 60, 30, 30 hidden units: Train set 85.48%, Test set 85.20%\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes =[ [50,30,20], [60,30,30]]\n",
    "for (n_h, n2, n3) in hidden_layer_sizes:\n",
    "    print (\"Hidden units:\", n_h, n2, n3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[-1]\n",
    "    parameters = initialize_parameters(n_x, n_h, n2, n3, n_y)\n",
    "    parameters = nn_model(X_train, Y_train, n_h, n2, n3, num_iterations = 50000)\n",
    "    train_predictions = predict(parameters, X_train)\n",
    "    test_predictions = predict(parameters, X_test)\n",
    "    train_accuracy = float((np.dot(Y_train[:,0],train_predictions) + np.dot(1-Y_train[:,0],1-train_predictions))/float(Y_train.size)*100)\n",
    "    test_accuracy = float((np.dot(Y_test[:,0],test_predictions) + np.dot(1-Y_test[:,0],1-test_predictions))/float(Y_test.size)*100)\n",
    "    print (\"  Accuracy for {}, {}, {} hidden units: Train set {:.2f}%, Test set {:.2f}%\".format(n_h, n2, n3, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "51f197c2-76d7-4503-ade5-f9f35323c4c5",
    "_uuid": "2e95a403934cf5c0ab1ff9148b62ef14590e84c9"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
